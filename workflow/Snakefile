##NSGTree (New SGTree)
##v0.41, August 2023
##fschulz@lbl.gov
"""
Build genome tree from concatenated alignment after hmmsearch using a set of user provided HMMs
No special characters in filename, no additional "."
Example run:
snakemake -j 24 --use-conda --config qfaadir="example" models="resources/models/rnapol.hmm" --configfile user_config.yml
"""
from pathlib import Path
import os
import sys

configfile: "workflow/config.yml"
# input and output dirs
qfaadir = Path(config["qfaadir"])
rfaadir = Path(config["rfaadir"])
qfaadir_base = os.path.basename(qfaadir)
rfaadir_base = os.path.basename(rfaadir)
modelscombined = Path(config["models"]) # file that contains all hmms
models_base = os.path.basename(modelscombined).split(".")[0]
analysisname = str(qfaadir_base + \
    "-" + rfaadir_base + \
    "-" + models_base + \
    "-" + config["tmethod"] + \
    "-perc" + str(int(float(config["minmarker"])*10))).replace(".", "")
outdir = config["qfaadir"] + "/nsgt_out/" + analysisname
reformatted_faa_dir = outdir + "/analyses/reformatted_faa/"

args = sys.argv
if "--configfile" in args:
  config_path = args[args.index("--configfile") + 1]
else:
  config_path = "workflow/config.yml"

# model names
MODELS = [line.replace("\n", "").split()[-1] for line in open(modelscombined, "r") if line.startswith("NAME")]

# global log file
workflow_log = outdir + "/workflow.log"

rule all:
  input:
    {workflow_log},
    expand(outdir + "/proteintrees/{model}.treefile", model=MODELS),
    outdir + "/" + analysisname + ".treefile",
    outdir + "/analyses.tar.gz"


rule help:
    input: 
        "workflow/Snakefile"
    params:
        configf = config_path,
        models = modelscombined,
        qfaa = qfaadir
    output:
        {workflow_log}
    shell:
        """
        (sed -n 's/^##//p' {input}
        date
        echo "################################"
        echo "Selected parameters for nsgtree"
        echo "models: {params.models}"
        echo "query dir: {params.qfaa}"
        cat {params.configf} | awk -F "#" '{{print $1}}') &> {output}
        echo "################################" >> {output}
        """


if config["rfaadir"]:
  rfaadir = Path(config["rfaadir"])
  FAABASENAMES = [x.stem for x in qfaadir.iterdir() if x.is_file() and x.suffix in [".faa"]] +\
    [x.stem for x in rfaadir.iterdir() if x.is_file() and x.suffix in [".faa"]]
  rule reformat_faa:
    """
    check format of faa input and reformat
    reformat headers to ><filenamebase>|<proteinid> 
    create summary stats, asssembly size, GC, coding density, gene count
    if faa provided add gene count only
    """
    conda:
      "envs/nsgt.yml"
    input:
      qdir = Path(config["qfaadir"]),
      rdir = Path(config["rfaadir"])
    output:
      reformatted_faa_dir = directory(outdir + "/analyses/reformatted_faa"),
      q_colors_itol = outdir + "/itol/" + models_base + ".colors.itol.txt",
      queries = outdir + "/itol/queries.txt"
    shell:
      """
      mkdir -p {output.reformatted_faa_dir}
      #cp {input.qdir}/*.faa {input.rdir}/*.faa {output.reformatted_faa_dir}
      echo "TREE_COLORS\nSEPARATOR COMMA\nDATA" > {output.q_colors_itol}
      ls {input.qdir} | grep -v "nsgt" | awk -F "." '{{print $1",branch,#cc0000,normal,1"}}' >> {output.q_colors_itol}
      ls {input.qdir} | grep -v "nsgt" | awk -F "." '{{print $1}}' > {output.queries}
      python workflow/scripts/reformat.py {input.rdir} {output.reformatted_faa_dir}
      python workflow/scripts/reformat.py {input.qdir} {output.reformatted_faa_dir}
      """
else:
  FAABASENAMES = [x.stem for x in qfaadir.iterdir() if x.is_file() and x.suffix in [".faa"]]
  rule reformat_faa:
    """
    check format of faa input and reformat
    reformat headers to ><filenamebase>|<proteinid> 
    create summary stats, asssembly size, GC, coding density, gene count
    if faa provided add gene count only
    """
    conda:
      "envs/nsgt.yml"
    input:
      qdir = Path(config["qfaadir"])
    output:
      reformatted_faa_dir = directory(outdir + "/analyses/reformatted_faa")
    shell:
      """
      mkdir -p {output.reformatted_faa_dir}
      python workflow/scripts/reformat.py {input.qdir} {output.reformatted_faa_dir}
      """


rule merge_qfaa:
  # merge all faa into one file 
  input:
    reformatted_faa_dir = outdir + "/analyses/reformatted_faa"
  params:
    faadirf = outdir + "/analyses/reformatted_faa"
  output:
    faacombined = outdir + "/analyses/tmp/merged.faa"
  shell:
    """
    cat {params.faadirf}/*.faa > {output.faacombined}
    echo "Number of genomes in the analysis: $(find {params.faadirf} -type f -name '*.faa' | wc -l)" >> {workflow_log}
    echo "################################" >> {workflow_log}
    """


rule run_hmmsearch:
  """
  hmmsearch to identify markers in query sequences
  """
  conda:
    "envs/nsgtt.yml"
  params:
    hmmsearch_cutoff = config["hmmsearch_cutoff"],
    hmmsearch_cpu = config["hmmsearch_cpu"]
  input:
    faacombined = outdir + "/analyses/tmp/merged.faa",
    modelshmm = str(modelscombined)
  log:
    outdir + "/analyses/log/hmmsearch/hmmsearch.log"
  output:
    hmmsearchout = outdir + "/analyses/hmmout/" + models_base + ".out"
  shell:
    """
    (echo "hmmsearch --noali {params.hmmsearch_cutoff} --domtblout  {output.hmmsearchout} --cpu {params.hmmsearch_cpu} {input.modelshmm} {input.faacombined}"
    hmmsearch --noali {params.hmmsearch_cutoff} --domtblout  {output.hmmsearchout} --cpu {params.hmmsearch_cpu} {input.modelshmm} {input.faacombined}) &> {log}
    """


rule run_hmmsearch_count_filter:
  """
  hmmsearch to identify markers in query sequences
  """
  conda:
    "envs/nsgt.yml"
  input:
    hmmsearchout = outdir + "/analyses/hmmout/" + models_base + ".out"
  params:
    faadirf = outdir + "/analyses/reformatted_faa",
    modelshmm = modelscombined,
    minmarker = config["minmarker"],
    maxsdup = config["maxsdup"],
    maxdupl = config["maxdupl"]
  log:
    outdir + "/analyses/log/hmmsearch/hmmsearch_filtering.log"
  output:
    hitcounts = outdir + "/analyses/hmmout/" + models_base + ".counts",
    taxa_removed = outdir + "/analyses/hmmout/" + models_base + ".removedtaxa",
    itolcounts = outdir + "/itol/" + models_base + ".counts.itol.txt"
  shell:
    """
    (echo "python workflow/scripts/hmmsearch_count_filter.py {params.faadirf} {params.modelshmm} {input.hmmsearchout} {output.hitcounts} {output.taxa_removed} {params.minmarker} {params.maxsdup} {params.maxdupl} {output.itolcounts}"
    python workflow/scripts/hmmsearch_count_filter.py {params.faadirf} {params.modelshmm} {input.hmmsearchout} {output.hitcounts} {output.taxa_removed} {params.minmarker} {params.maxsdup} {params.maxdupl} {output.itolcounts}) &> {log}
    echo "Number of genomes that were removed due to filtering thresholds: $(wc -l <{output.taxa_removed}) " >> {workflow_log}
    cat {output.taxa_removed} >> {workflow_log}
    """


rule extract_qhits:
  """
  extract hits from hmmsearch, one file per marker, merge with refs
  """
  conda:
    "envs/nsgt.yml"
  log:
    outdir + "/analyses/log/extraction/{model}.log"
  params:
    faadirf = outdir + "/analyses/reformatted_faa",
    extract_processes = config["extract_processes"],
    modelname = "{model}"
  input:
    hmmsearchout = outdir + "/analyses/hmmout/" + models_base + ".out",
    taxa_removed = outdir + "/analyses/hmmout/" + models_base + ".removedtaxa"
  output:
    outdir + "/analyses/hits_faa/{model}.faa"
  shell:
    """
    (echo "python workflow/scripts/extract_qhits.py {input.hmmsearchout} {params.faadirf} {params.extract_processes} {input.taxa_removed} {output}"
    python workflow/scripts/extract_qhits.py {input.hmmsearchout} {params.faadirf} {params.extract_processes} {input.taxa_removed} {output}) &> {log}
    declare -i protein_count
    if [ ! -s {output} ]; then
      protein_count=0
    else
      protein_count=$(grep -c ">" {output})
    fi
    echo "Extracted proteins from $protein_count different taxa for {params.modelname}" >> {log}
    touch {output}
    """


rule align_trim:
  """
  align extracted GVOGs together with refs
  """
  conda:
    "envs/nsgtt.yml"
  log:
    outdir + "/analyses/log/aln/{model}.log"
  params:
    mafft_thread = config["mafft_thread"],
    mafftv = config["mafftv"],
    mafft = config["mafft"],
    trimal_gt = config["trimal_gt"]
  input:
    hitsfaa = outdir + "/analyses/hits_faa/{model}.faa"
  output:
    aln = outdir + "/analyses/aligned/{model}.mafft",
    trimmedaln = outdir + "/analyses/aligned_t/{model}.mafft_t"
  shell:
    """
    (echo "mafft{params.mafftv} {params.mafft} --quiet {params.mafft_thread} {input.hitsfaa} > {output.aln}"
    mafft{params.mafftv} {params.mafft} --quiet {params.mafft_thread} {input.hitsfaa} > {output.aln} || touch {output.aln}
    echo "trimal {params.trimal_gt} -in {output.aln} -out {output.trimmedaln}"
    trimal {params.trimal_gt} -in {output.aln} -out {output.trimmedaln} || touch {output.trimmedaln} ) >& {log}
    """


if config["tmethod"] == "fasttree":
  rule build_trees:
    """build single protein trees for each marker"""
    conda:
      "envs/nsgtt.yml"
    log:
      outdir + "/analyses/log/trees/{model}.log"
    input:
      trimmedaln = outdir + "/analyses/aligned_t/{model}.mafft_t"
    params:
      proteintrees = config["ft_proteintrees"],
      outdirt = outdir + "/analyses/proteintrees/",
      outpath = outdir + "/analyses/proteintrees/{model}"
    output:
      tree = outdir + "/proteintrees/{model}.treefile"
    shell:
      """
      (echo "fasttree {params.proteintrees} {input.trimmedaln} > {output.tree}"
      mkdir -p {params.outdirt}
      fasttree {params.proteintrees} {input.trimmedaln} > {params.outpath}.treefile || touch {output.tree}
      cp {params.outpath}.treefile {output.tree}) >& {log}
      """
elif config["tmethod"] == "iqtree":
  rule build_trees:
    """
    build single protein trees for each marker
    """
    conda:
      "envs/nsgtt.yml"
    log:
      outdir + "/analyses/log/trees/{model}.log"
    input:
      trimmedaln = outdir + "/analyses/aligned_t/{model}.mafft_t"
    params:
      proteintrees = config["iq_proteintrees"],
      outdirt = outdir + "/analyses/proteintrees/iqtree",
      outpath = outdir + "/analyses/proteintrees/iqtree/{model}"
    output:
      tree = outdir + "/proteintrees/{model}.treefile"
    shell:
      """
      (echo "iqtree --prefix {params.outpath} -m {params.proteintrees} -T 1 -fast -s {input.trimmedaln}"
      mkdir -p {params.outdirt}
      iqtree --prefix {params.outpath} -m {params.proteintrees} -T 1 -fast -s {input.trimmedaln} || touch {params.outpath}.treefile
      cp {params.outpath}.treefile {output.tree}
      touch {output.tree}) >& {log}
      """


rule concat_aln:
  conda:
    "envs/nsgt.yml"
  log:
    outdir + "/analyses/log/aln/concat.log"
  input:
    expand(outdir + "/analyses/aligned_t/{model}.mafft_t", model=MODELS)
  params:
    alndir = outdir + "/analyses/aligned_t/"
  output:
    concataln = outdir + "/" + analysisname + ".mafft_t"
  shell:
    """
    (python workflow/scripts/concat.py {params.alndir} {output.concataln} || touch {output.concataln}) >& {log}
    echo "################################" >> {workflow_log}
    echo "Number of genomes in the final alignment: $(grep -c '>' {output.concataln})" >> {workflow_log}
    """


if config["tmethod"] == "fasttree":
  rule species_tree:
    conda:
      "envs/nsgtt.yml"
    log:
      outdir + "/analyses/log/trees/speciestree.log"
    params:
      speciestree = config["ft_speciestree"],
      outpath = outdir + "/analyses/finaltree/fasttree/" + analysisname
    input:
      concataln = outdir + "/" + analysisname + ".mafft_t"
    output:
      outdir + "/analyses/finaltree/fasttree/" + analysisname + ".treefile",
      concattree = outdir + "/" + analysisname + ".treefile"
    shell:
      """
      declare -i genome_count
      if [ ! -s {input.concataln} ]; then
        genome_count=0
      else
        genome_count=$(grep -c ">" {input.concataln})
      fi

      if [ $genome_count -le 3 ]; then
        error_message="Too few genomes in the supermatrix alignment. Check filtering thresholds to increase numbers of genomes to be retained."
        echo $error_message >> {log}
        echo $error_message >&2
        echo $error_message >> {workflow_log}
        exit 1
      fi
      echo "fasttree {params.speciestree} {input.concataln} > {output[0]}" >> {log}
      fasttree {params.speciestree} {input.concataln} > {output[0]} 
      cp {output[0]} {output[1]}
      """
elif config["tmethod"] == "iqtree":
  rule species_tree:
    conda:
      "envs/nsgtt.yml"
    log:
      outdir + "/analyses/log/trees/speciestree.log"
    params:
      speciestree = config["iq_speciestree"],
      outpath = outdir + "/analyses/finaltree/iqtree/" + analysisname
    input:
      concataln = outdir + "/" + analysisname + ".mafft_t"
    output:
      outdir + "/analyses/finaltree/iqtree/" + analysisname + ".treefile",
      concattree = outdir + "/" + analysisname + ".treefile"
    shell:
      """
      declare -i genome_count
      if [ ! -s {input.concataln} ]; then
        genome_count=0
      else
        genome_count=$(grep -c ">" {input.concataln})
      fi

      if [ $genome_count -le 3 ]; then
        error_message="Too few genomes in the supermatrix alignment. Check filtering thresholds to increase numbers of genomes to be retained."
        echo $error_message >> {log}
        echo $error_message >&2
        echo $error_message >> {workflow_log}
        exit 1
      fi
      echo "iqtree --quiet --prefix {params.outpath} -m {params.speciestree} -fast -s {input.concataln}" >> {log}
      iqtree --quiet --prefix {params.outpath} -m {params.speciestree} -fast -s {input.concataln} >> {log} 2>&1
      cp {output[0]} {output[1]}
      """

rule cleanup:
  """
  intermediate files are kept including empty files, compress output 
  """
  conda:
    "envs/ete3.yml"
  input:
    outdir + "/" + analysisname + ".treefile",
    expand(outdir + "/proteintrees/{model}.treefile", model=MODELS)
  params:
    fdir = outdir + "/analyses",
    itoldir = outdir + "/itol",
    q_faa_f_dir = outdir + "/analyses/reformatted_faa/",
    tmp_dir = outdir + "/analyses/tmp"
  output:
    outdir + "/analyses.tar.gz"
  shell:
    """
    if [ -e {params.itoldir}/queries.txt ]; then python workflow/scripts/ete3_nntree.py {input[0]} {params.itoldir}/queries.txt {params.itoldir}/nn.itol.txt ; fi
    if [ -e {params.itoldir}/queries.txt ]; then python workflow/scripts/ete3_clademembers.py {input[0]} {params.itoldir}/queries.txt cc0000 {params.itoldir}/clademembers_colored.itol.txt
    sed -i '1 i\DATA' {params.itoldir}/clademembers_colored.itol.txt
    sed -i '1 i\SEPARATOR COMMA' {params.itoldir}/clademembers_colored.itol.txt
    sed -i '1 i\TREE_COLOR' {params.itoldir}/clademembers_colored.itol.txt ; fi
    rm -r {params.q_faa_f_dir}
    rm -r {params.tmp_dir}
    find {params.fdir} -type f -empty -print -delete 
    python workflow/scripts/cleanup.py {params.fdir} {output}
    """
